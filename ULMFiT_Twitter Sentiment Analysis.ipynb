{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlekseiMikhalev/Data-Science-Projects/blob/master/ULMFiT_Twitter%20Sentiment%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3m9TOWRGZhq",
        "colab_type": "text"
      },
      "source": [
        "## This notebook has been created for participation in Fellowship.ai challenge. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6JFDwB8G0zn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f190f9af-1fdf-4229-e4ef-fe0346531d50"
      },
      "source": [
        "# Environment setup. Importing the libraries.\n",
        "import re\n",
        "from functools import partial\n",
        "from collections import Counter\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "stop_words = stopwords.words('english')\n",
        "from fastai.text import *\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu7cEnXAOm5R",
        "colab_type": "text"
      },
      "source": [
        "Importing dataset and doing data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynkd4mgcOcHh",
        "colab_type": "code",
        "outputId": "750c9b1e-41ea-493c-f665-45e8ed419937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "# Import the file with dataset and read the first five rows to get the overview of the dataset\n",
        "tweet = pd.read_csv(\"/content/Tweets.csv\")\n",
        "tweet.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW3SKSQGQhLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "35fe02cb-fd05-41ce-e811-43813e26a7bb"
      },
      "source": [
        "# Explore the number of each feature\n",
        "tweet.count()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tweet_id                        14640\n",
              "airline_sentiment               14640\n",
              "airline_sentiment_confidence    14640\n",
              "negativereason                   9178\n",
              "negativereason_confidence       10522\n",
              "airline                         14640\n",
              "airline_sentiment_gold             40\n",
              "name                            14640\n",
              "negativereason_gold                32\n",
              "retweet_count                   14640\n",
              "text                            14640\n",
              "tweet_coord                      1019\n",
              "tweet_created                   14640\n",
              "tweet_location                   9907\n",
              "user_timezone                    9820\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGXL_PY_7BSd",
        "colab_type": "text"
      },
      "source": [
        "We need to preprocess our dataset and transform it into the dataset which is appropriate for further use with ULMFiT. Our new dataset should contain label and text columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppgftPTA7Z6A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "c050ab0a-08a4-4918-dea1-1f43d89e2d91"
      },
      "source": [
        "# Removing needless columns and create a new dataset \n",
        "needless_column = tweet[['tweet_id','airline_sentiment_gold','negativereason_gold','retweet_count','tweet_coord','tweet_location','user_timezone', 'name', 'tweet_created']]\n",
        "new_tweet = tweet.drop(columns=needless_column)\n",
        "new_tweet.head()\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  airline_sentiment  ...                                               text\n",
              "0           neutral  ...                @VirginAmerica What @dhepburn said.\n",
              "1          positive  ...  @VirginAmerica plus you've added commercials t...\n",
              "2           neutral  ...  @VirginAmerica I didn't today... Must mean I n...\n",
              "3          negative  ...  @VirginAmerica it's really aggressive to blast...\n",
              "4          negative  ...  @VirginAmerica and it's a really big bad thing...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz6GcEe7MkSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save new DataFrame into csv file for the fast.ai processing\n",
        "new_tweet.to_csv(r'/content/new_tweet.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xncW466ZVHra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "86c771c9-83c4-470e-e763-d4a4dbc7c9f5"
      },
      "source": [
        "# Clean the data for modelling \n",
        "# Retaining only alphabets and removing everything else\n",
        "new_tweet['text'] = new_tweet['text'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "new_tweet['text'].head()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   VirginAmerica What  dhepburn said \n",
              "1     VirginAmerica plus you ve added commercials t...\n",
              "2     VirginAmerica I didn t today    Must mean I n...\n",
              "3     VirginAmerica it s really aggressive to blast...\n",
              "4     VirginAmerica and it s a really big bad thing...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr0f2trjWCqV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4adf1ebe-ec67-43d6-b91c-10596dda8d9f"
      },
      "source": [
        "# Now we are getting rid of the stopwords from the text data by using nltk package\n",
        "# Tokenization \n",
        "tokenized_new_tweet = new_tweet['text'].apply(lambda x: x.split())\n",
        "\n",
        "# Remove stop-words \n",
        "tokenized_new_tweet = tokenized_new_tweet.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "\n",
        "# De-tokenization \n",
        "detokenized_new_tweet = [] \n",
        "for i in range(len(new_tweet)): \n",
        "    t = ' '.join(tokenized_new_tweet[i]) \n",
        "    detokenized_new_tweet.append(t) \n",
        "\n",
        "new_tweet['text'] = detokenized_new_tweet\n",
        "new_tweet.head()\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>VirginAmerica What dhepburn said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>VirginAmerica plus added commercials experienc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>VirginAmerica I today Must mean I need take an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>VirginAmerica really aggressive blast obnoxiou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>VirginAmerica really big bad thing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  airline_sentiment  ...                                               text\n",
              "0           neutral  ...                   VirginAmerica What dhepburn said\n",
              "1          positive  ...  VirginAmerica plus added commercials experienc...\n",
              "2           neutral  ...  VirginAmerica I today Must mean I need take an...\n",
              "3          negative  ...  VirginAmerica really aggressive blast obnoxiou...\n",
              "4          negative  ...                 VirginAmerica really big bad thing\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6qXN4d1B4Fp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "6f75a2fb-6cc8-4ccf-a3b6-051996d4e82e"
      },
      "source": [
        "'''# Pre-processing data for modelling\n",
        "# Language model data\n",
        "data_lm = TextLMDataBunch.from_csv(path=\"/content/\", csv_name='new_tweet.csv')\n",
        "# Classifier model data\n",
        "data_clas = TextClasDataBunch.from_csv(path=\"/content/\", csv_name='new_tweets.csv', vocab=data_lm.train_ds.vocab, bs=32)'''"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/fastai/data_block.py:537: UserWarning: You are labelling your items with CategoryList.\n",
            "Your valid set contained the following unknown labels, the corresponding items have been discarded.\n",
            "3196, 4884, 4795, 12708, 358...\n",
            "  if getattr(ds, 'warn', False): warn(ds.warn)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L90Qm-98RJTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''# split data into training and validation set\n",
        "df_trn, df_val = train_test_split(df, stratify = df['label'], test_size = 0.4, random_state = 12)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9fcEc4kSF-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''df_trn.shape, df_val.shape'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAOxB1CybgCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Before proceeding further, we’ll need to prepare our data for the language model and for the classification model separately. The good news? This can be done quite easily using the fastai library:\n",
        "\n",
        "# Language model data\n",
        "data_lm = TextLMDataBunch.from_df(train_df = df_trn, valid_df = df_val, path = \"\")\n",
        "\n",
        "# Classifier model data\n",
        "data_clas = TextClasDataBunch.from_df(path = \"\", train_df = df_trn, valid_df = df_val, vocab=data_lm.train_ds.vocab, bs=32)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkLiDhcTbp7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Fine-Tuning the Pre-Trained Model and Making Predictions\n",
        "We can use the data_lm object we created earlier to fine-tune a pre-trained language model. We can create a learner object, ‘learn’, that will directly create a model, download the pre-trained weights, and be ready for fine-tuning:\n",
        "\n",
        "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.7)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi5MB5lYb7T5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''We will save this encoder to use it for classification later.\n",
        "\n",
        "learn.save_encoder('ft_enc')\n",
        "Let’s now use the data_clas object we created earlier to build a classifier with our fine-tuned encoder.\n",
        "\n",
        "learn = text_classifier_learner(data_clas, drop_mult=0.7)\n",
        "learn.load_encoder('ft_enc')\n",
        "\n",
        "We will again try to fit our model.\n",
        "\n",
        "learn.fit_one_cycle(1, 1e-2)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ogzeQVycJR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Wow! We got a whopping increase in the accuracy and even the validation loss is far less than the training loss. It is a pretty outstanding performance on a small dataset. You can even get the predictions for the validation set out of the learner object by using the below code:\n",
        "\n",
        "# get predictions\n",
        "preds, targets = learn.get_preds()\n",
        "\n",
        "predictions = np.argmax(preds, axis = 1)\n",
        "pd.crosstab(predictions, targets)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-w0hxANcejq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}